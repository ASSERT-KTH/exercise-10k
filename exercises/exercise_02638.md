## Exercise
Create a program that takes an image description as input and generates a caption.

## Reference Solution
```python
import nltk
from nltk.tokenize import word_tokenize
from transformers import AutoModelForSequenceClassification, AutoTokenizer

# Load pre-trained model and tokenizer
model_name = "distilbert-base-uncased"
model = AutoModelForSequenceClassification.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

def generate_caption(image_description):
    """
    Generate a caption based on the image description.

    Args:
        image_description (str): The description of the image.

    Returns:
        str: The generated caption.
    """
    # Tokenize the image description
    tokens = word_tokenize(image_description)

    # Convert tokens to input IDs and attention mask
    inputs = tokenizer(tokens, return_tensors="pt", max_length=512, truncation=True, padding="max_length")

    # Generate caption using the pre-trained model
    outputs = model.generate(inputs["input_ids"], attention_mask=inputs["attention_mask"])

    # Convert output IDs to caption text
    caption = tokenizer.decode(outputs[0], skip_special_tokens=True)

    return caption

# Example usage
image_description = "A beautiful sunset over a serene lake with a few trees in the foreground."
caption = generate_caption(image_description)
print("Generated Caption:", caption)
```

## Generated Output
```
/opt/conda/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.
  warnings.warn(
Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Traceback (most recent call last):
  File "/tmp/tmpoaz3ffgy.py", line 36, in <module>
    caption = generate_caption(image_description)
  File "/tmp/tmpoaz3ffgy.py", line 27, in generate_caption
    outputs = model.generate(inputs["input_ids"], attention_mask=inputs["attention_mask"])
  File "/opt/conda/lib/python3.10/site-packages/torch/utils/_contextlib.py", line 115, in decorate_context
    return func(*args, **kwargs)
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 1279, in generate
    self._validate_model_class()
  File "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py", line 1065, in _validate_model_class
    raise TypeError(exception_message)
TypeError: The current model class (DistilBertForSequenceClassification) is not compatible with `.generate()`, as it doesn't have a language model head.
```

generated by python 3.10.13

solution generated by RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
