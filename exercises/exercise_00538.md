## Exercise
Implement the Viterbi algorithm to find the most likely state sequence in a hidden Markov model.

## Reference Solution
```python
import numpy as np

def viterbi(obs, states, start_p, trans_p, emit_p):
    """
    Viterbi algorithm to find the most likely state sequence.

    Parameters:
    - obs (list): Observation sequence.
    - states (list): List of states in the HMM.
    - start_p (dict): Start probabilities for each state.
    - trans_p (dict): Transition probabilities between states.
    - emit_p (dict): Emission probabilities for each state.

    Returns:
    - most_likely_sequence (list): The most likely state sequence.
    """
    V = [{}]
    for state in states:
        # Initialize the first time step
        V[0][state] = {"prob": start_p[state] * emit_p[state][obs[0]], "prev": None}

    # Iterate over the observation sequence
    for t in range(1, len(obs)):
        V.append({})
        for state in states:
            # Initialize the maximum probability and previous state
            max_prob = float("-inf")
            prev_state = None
            for prev in states:
                # Calculate the probability of transitioning from the previous state
                prob = V[t - 1][prev]["prob"] * trans_p[prev][state] * emit_p[state][obs[t]]
                if prob > max_prob:
                    max_prob = prob
                    prev_state = prev
            # Update the current state's probability and previous state
            V[t][state] = {"prob": max_prob, "prev": prev_state}

    # Find the most likely final state
    max_prob = float("-inf")
    prev_state = None
    for state in states:
        if V[-1][state]["prob"] > max_prob:
            max_prob = V[-1][state]["prob"]
            prev_state = state

    # Backtrack to find the most likely state sequence
    most_likely_sequence = []
    t = len(obs) - 1
    while t >= 0:
        most_likely_sequence.append(prev_state)
        prev_state = V[t][prev_state]["prev"]
        t -= 1
    most_likely_sequence.reverse()

    return most_likely_sequence

# Example usage:
if __name__ == "__main__":
    # Define the HMM parameters
    states = ["Healthy", "Fever"]
    obs = ["normal", "cold", "dizzy"]
    start_p = {"Healthy": 0.6, "Fever": 0.4}
    trans_p = {
        "Healthy": {"Healthy": 0.7, "Fever": 0.3},
        "Fever": {"Healthy": 0.4, "Fever": 0.6},
    }
    emit_p = {
        "Healthy": {"normal": 0.5, "cold": 0.4, "dizzy": 0.1},
        "Fever": {"normal": 0.1, "cold": 0.3, "dizzy": 0.6},
    }

    # Run the Viterbi algorithm
    most_likely_sequence = viterbi(obs, states, start_p, trans_p, emit_p)
    print("Most likely state sequence:", most_likely_sequence)
```

## Generated Output
```
Most likely state sequence: ['Healthy', 'Healthy', 'Fever']
```

generated by python 3.10.13

solution generated by RedHatAI/Llama-3.3-70B-Instruct-FP8-dynamic
